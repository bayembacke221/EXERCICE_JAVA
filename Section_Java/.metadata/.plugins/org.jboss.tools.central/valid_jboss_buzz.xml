<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>How to patch modules in Red Hat Enterprise Linux</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rIvhVOaOwzQ/how-patch-modules-red-hat-enterprise-linux" /><author><name>Petr Pisar</name></author><id>ead51897-4c95-47c9-9b11-cf8ff1f05c6c</id><updated>2021-08-03T07:00:00Z</updated><published>2021-08-03T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL), in version 8, introduced &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#introduction-to-modules_using-appstream"&gt;modules&lt;/a&gt; as a higher-level concept for packaging software stacks. Modules enable new features such as adding alternative versions of stacks, called &lt;em&gt;streams&lt;/em&gt;. That's great, but what if you want to patch a stream? Is it possible? It is. Is it more difficult than patching non-modular software? Slightly. This article shows you how to patch a module stream while avoiding the invisible package problem.&lt;/p&gt; &lt;h2&gt;Patching a module in RHEL&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux is open source. That means you can take the code sources, change them, recompile them, and use or redistribute the modified software. As an example, we can change the HTTPD web server to report a different server name in the HTTP response headers.&lt;/p&gt; &lt;p&gt;To get started, install an &lt;code&gt;httpd&lt;/code&gt; RPM package, start the HTTPD server, and check the server name. I have highlighted the relevant lines from the output in bold:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# yum install httpd Last metadata expiration check: 0:03:40 ago on Fri 16 Jul 2021 12:51:49 PM CEST. Dependencies resolved. ========================================================================================== Package Arch Version Repository Size ========================================================================================== Installing: &lt;strong&gt;httpd x86_64 2.4.37-40.module+el8.5.0+11022+1c90597b&lt;/strong&gt; rhel-8.5.0-appstream 1.4 M Installing dependencies: httpd-filesystem noarch 2.4.37-40.module+el8.5.0+11022+1c90597b rhel-8.5.0-appstream 39 k httpd-tools x86_64 2.4.37-40.module+el8.5.0+11022+1c90597b rhel-8.5.0-appstream 106 k mod_http2 x86_64 1.15.7-3.module+el8.4.0+8625+d397f3da pulp-appstream 154 k redhat-logos-httpd noarch 84.5-1.el8 rhel-8.5.0-baseos 29 k Enabling module streams: &lt;strong&gt;httpd 2.4&lt;/strong&gt; Transaction Summary ========================================================================================== Install 5 Packages Total download size: 1.7 M Installed size: 4.9 M Is this ok [y/N]: y […] Complete! # systemctl start httpd $ wget --no-proxy -S -O /dev/null http://localhost/ --2021-07-16 12:58:54-- http://localhost/ Resolving localhost (localhost)... ::1, 127.0.0.1 Connecting to localhost (localhost)|::1|:80... connected. HTTP request sent, awaiting response... HTTP/1.1 403 Forbidden Date: Fri, 16 Jul 2021 10:58:54 GMT &lt;strong&gt;Server: Apache/2.4.37 (Red Hat Enterprise Linux)&lt;/strong&gt; Last-Modified: Mon, 12 Jul 2021 19:36:32 GMT ETag: "133f-5c6f23d09f000" Accept-Ranges: bytes Content-Length: 4927 Keep-Alive: timeout=5, max=100 Connection: Keep-Alive Content-Type: text/html; charset=UTF-8 2021-07-16 12:58:54 ERROR 403: Forbidden.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows that the &lt;code&gt;httpd-2.4.37-40.module+el8.5.0+11022+1c90597b&lt;/code&gt; RPM package was installed from the &lt;code&gt;httpd:2.4&lt;/code&gt; module stream and that the server reports &lt;code&gt;Apache/2.4.37 (Red Hat Enterprise Linux)&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Our quest is to patch the module to report &lt;code&gt;My Linux&lt;/code&gt; instead.&lt;/p&gt; &lt;h2&gt;Step 1: Build a new package&lt;/h2&gt; &lt;p&gt;First, obtain the source RPM package, &lt;code&gt;httpd-2.4.37-40.module+el8.5.0+11022+1c90597b.src.rpm&lt;/code&gt;, which corresponds to our example. Unpack it and apply the following patch to a specification file, as explained in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/packaging_and_distributing_software/index#patching-software_preparing-software-for-rpm-packaging"&gt;Red Hat documentation&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- a/httpd.spec +++ b/httpd.spec @@ -13,7 +13,7 @@ Summary: Apache HTTP Server Name: httpd Version: 2.4.37 -Release: 40%{?dist} +Release: 41%{?dist} URL: https://httpd.apache.org/ Source0: https://www.apache.org/dist/httpd/httpd-%{version}.tar.bz2 Source2: httpd.logrotate @@ -370,7 +370,7 @@ interface for storing and accessing per-user session data. %patch211 -p1 -b .CVE-2020-11984 # Patch in the vendor string -sed -i '/^#define PLATFORM/s/Unix/%{vstring}/' os/unix/os.h +sed -i '/^#define PLATFORM/s/Unix/My Linux/' os/unix/os.h sed -i 's/@RELEASE@/%{release}/' server/core.c # Prevent use of setcap in "install-suexec-caps" target. @@ -870,6 +870,9 @@ rm -rf $RPM_BUILD_ROOT %{_rpmconfigdir}/macros.d/macros.httpd %changelog +* Wed Jun 23 2021 Petr Pisar &lt;ppisar@redhat.com&gt; - 2.4.37-41 +- Modified server platform + * Fri May 14 2021 Lubos Uhliarik &lt;luhliari@redhat.com&gt; - 2.4.37-40 - Resolves: #1952557 - mod_proxy_wstunnel.html is a malformed XML - Resolves: #1937334 - SSLProtocol with based virtual hosts&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/packaging_and_distributing_software/index#building-binary-rpms_building-rpms"&gt;build the modified package&lt;/a&gt; with a &lt;code&gt;rpmbuild&lt;/code&gt; tool. This results in the following binary packages:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ls httpd-2.4.37-41.el8.x86_64.rpm mod_ldap-2.4.37-41.el8.x86_64.rpm httpd-debuginfo-2.4.37-41.el8.x86_64.rpm mod_ldap-debuginfo-2.4.37-41.el8.x86_64.rpm httpd-debugsource-2.4.37-41.el8.x86_64.rpm mod_proxy_html-2.4.37-41.el8.x86_64.rpm httpd-devel-2.4.37-41.el8.x86_64.rpm mod_proxy_html-debuginfo-2.4.37-41.el8.x86_64.rpm httpd-filesystem-2.4.37-41.el8.noarch.rpm mod_session-2.4.37-41.el8.x86_64.rpm httpd-manual-2.4.37-41.el8.noarch.rpm mod_session-debuginfo-2.4.37-41.el8.x86_64.rpm httpd-tools-2.4.37-41.el8.x86_64.rpm mod_ssl-2.4.37-41.el8.x86_64.rpm httpd-tools-debuginfo-2.4.37-41.el8.x86_64.rpm mod_ssl-debuginfo-2.4.37-41.el8.x86_64.rpm &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 2: Create a nonmodular repository&lt;/h2&gt; &lt;p&gt;Next, turn the directory into a YUM repository. Let's say that the repository is located in the working directory &lt;code&gt;/root/repos/myhttpd&lt;/code&gt;, so all write operations there must be performed by a superuser:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# createrepo_c . Directory walk started Directory walk done - 16 packages Temporary output repo path: ./.repodata/ Preparing sqlite DBs Pool started (with 5 workers) Pool finished&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Register the repository to YUM under the name &lt;code&gt;myhttpd&lt;/code&gt; by creating the &lt;code&gt;/etc/yum.repos.d/devel.repo&lt;/code&gt; file with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[myhttpd] name=myhttpd packages baseurl=file:///root/repos/myhttpd/ enabled=1 gpgcheck=0&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;The invisible package problem&lt;/h3&gt; &lt;p&gt;Next, let's try to update the system to install the patched package:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# yum upgrade myhttpd packages 2.9 MB/s | 3.0 kB 00:00 myhttpd packages 2.5 MB/s | 25 kB 00:00 Dependencies resolved. Nothing to do. Complete!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It doesn't work! YUM cannot see your new &lt;code&gt;httpd-2.4.37-41.el8.x86_64&lt;/code&gt; package. Check which packages YUM sees:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ repoquery httpd Last metadata expiration check: 0:06:19 ago on Fri 16 Jul 2021 01:31:22 PM CEST. httpd-0:2.4.37-10.module+el8+2764+7127e69e.x86_64 httpd-0:2.4.37-11.module+el8.0.0+2969+90015743.x86_64 httpd-0:2.4.37-12.module+el8.0.0+4096+eb40e6da.x86_64 httpd-0:2.4.37-16.module+el8.1.0+4134+e6bad0ed.x86_64 httpd-0:2.4.37-21.module+el8.2.0+5008+cca404a3.x86_64 httpd-0:2.4.37-30.module+el8.3.0+7001+0766b9e7.x86_64 httpd-0:2.4.37-39.module+el8.4.0+9658+b87b2deb.x86_64 httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The new package isn't there, but why? Because packages belonging to an active module stream take precedence over other packages of the same name. This issue is sometimes known as the invisible package problem. We'll explore it in the next section.&lt;/p&gt; &lt;h3&gt;Theory of modules&lt;/h3&gt; &lt;p&gt;To resolve the invisible package problem, you need to understand how modules work.&lt;/p&gt; &lt;p&gt;Modules are organized into streams (examples include &lt;code&gt;httpd:2.4&lt;/code&gt;, &lt;code&gt;perl:5.24&lt;/code&gt;, and &lt;code&gt;perl:5.30&lt;/code&gt;; also see the &lt;code&gt;yum module list&lt;/code&gt; command output). Each stream consists of a series of module versions (such as &lt;code&gt;httpd:2.4:8040020210127115317&lt;/code&gt; and &lt;code&gt;httpd:2.4:8050020210517115912&lt;/code&gt;) and each module version lists RPM packages belonging to it (see the &lt;code&gt;Artifacts&lt;/code&gt; section in the output from &lt;code&gt;yum module info httpd:2.4&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;A module stream is &lt;em&gt;active&lt;/em&gt; if the developer enables it explicitly, or if it is the default and has not been explicitly disabled. All packages belonging to the active stream are visible to YUM. All other packages of the same name, including packages not belonging to any module, are invisible.&lt;/p&gt; &lt;p&gt;Correspondingly, when a module stream is &lt;em&gt;not active&lt;/em&gt; (is disabled or is nondefault), its packages are invisible, while nonmodular packages with the same name are kept visible.&lt;/p&gt; &lt;p&gt;In a typical Red Hat Enterprise Linux distribution, you can observe changes in visibility as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Enable the &lt;code&gt;perl:5.24&lt;/code&gt; stream by running &lt;code&gt;yum enable perl:5.24&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;List the Perl packages in the repository by entering &lt;code&gt;repoquery perl&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Reset the stream through &lt;code&gt;yum module reset perl&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Enable &lt;code&gt;perl:5.30&lt;/code&gt; in a similar way.&lt;/li&gt; &lt;li&gt;List the packages again and view the differences from the previous listing.&lt;/li&gt; &lt;li&gt;Reset the stream to &lt;code&gt;perl:5.26&lt;/code&gt;, or whatever the default was on your system.&lt;/li&gt; &lt;li&gt;List the packages again.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;After each change, YUM lists different &lt;code&gt;perl&lt;/code&gt; packages.&lt;/p&gt; &lt;h3&gt;Solving the invisible package problem&lt;/h3&gt; &lt;p&gt;In our example, the new &lt;code&gt;httpd-2.4.37-41.el8.x86_64&lt;/code&gt; is currently prevented from being visible. The &lt;code&gt;httpd:2.4&lt;/code&gt; module stream is active and lists an &lt;code&gt;httpd&lt;/code&gt; package:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ yum module info httpd:2.4 […] Name : httpd Stream : 2.4 [d][e][a] Version : 8050020210517115912 Context : b4937e53 Architecture : x86_64 Profiles : common [d], devel, minimal Default profiles : common Repo : rhel-8.5.0-appstream Summary : Apache HTTP Server Description : Apache httpd is a powerful, efficient, and extensible HTTP server. Requires : platform:[el8] Artifacts : httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.src : &lt;strong&gt;httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64&lt;/strong&gt; […]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To patch the module, you need to define a new &lt;code&gt;httpd:2.4&lt;/code&gt; module version, list the new &lt;code&gt;httpd-2.4.37-41.el8.x86_64&lt;/code&gt; package there, and add the new module version definition to the repository. After you've done that, YUM will recognize the new package as belonging to the &lt;code&gt;httpd:2.4&lt;/code&gt; stream, and you can continue to the next step.&lt;/p&gt; &lt;h2&gt;Step 3: Make the repository modular&lt;/h2&gt; &lt;p&gt;Now comes a step specific to modules: Changing a nonmodular repository into a modular one. Copy the &lt;code&gt;httpd:2.4:8050020210517115912:b4937e53:x86_64&lt;/code&gt; module definition from the original repository to the &lt;code&gt;modules.yaml&lt;/code&gt; file in the directory with the new package:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# zcat /var/cache/dnf/rhel-8.5.0-appstream-801b3acbf7fb96cf/repodata/7642b0bd7a55141335285144eb537352c85f336de8187ad14aa40b0dbf532463-modules.yaml.gz &gt; modules.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I took the file from a local YUM cache. But you can also find files with names matching &lt;code&gt;*-modules.yaml*&lt;/code&gt; on repository mirrors.&lt;/p&gt; &lt;p&gt;Now, locate the module build definition inside the file and delete everything else:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- document: modulemd version: 2 data: name: httpd stream: "2.4" version: 8050020210517115912 context: b4937e53 arch: x86_64 […] artifacts: rpms: - httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.src - httpd-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-debuginfo-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-debugsource-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-devel-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-filesystem-0:2.4.37-40.module+el8.5.0+11022+1c90597b.noarch - httpd-manual-0:2.4.37-40.module+el8.5.0+11022+1c90597b.noarch - httpd-tools-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - httpd-tools-debuginfo-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_http2-0:1.15.7-3.module+el8.4.0+8625+d397f3da.src - mod_http2-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_http2-debuginfo-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_http2-debugsource-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_ldap-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_ldap-debuginfo-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_md-1:2.0.8-8.module+el8.3.0+6814+67d1e611.src - mod_md-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_md-debuginfo-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_md-debugsource-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_proxy_html-1:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_proxy_html-debuginfo-1:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_session-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_session-debuginfo-0:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_ssl-1:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 - mod_ssl-debuginfo-1:2.4.37-40.module+el8.5.0+11022+1c90597b.x86_64 ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Update the RPM package list in the &lt;code&gt;/data/artifacts/rpms&lt;/code&gt; YAML node to match your new RPM builds:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# sed -i -e 's/-40\.module+el8\.5\.0+11022+1c90597b\./-41.el8./' modules.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Increment the module build version; for example, from &lt;code&gt;8050020210517115912&lt;/code&gt; to &lt;code&gt;8050020210517115913&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- document: modulemd version: 2 data: name: httpd stream: "2.4" version: &lt;strong&gt;8050020210517115913&lt;/strong&gt; context: b4937e53 arch: x86_64 [...] artifacts: rpms: - httpd-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.src - httpd-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-debuginfo-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-debugsource-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-devel-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-filesystem-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.noarch - httpd-manual-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.noarch - httpd-tools-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - httpd-tools-debuginfo-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_http2-0:1.15.7-3.module+el8.4.0+8625+d397f3da.src - mod_http2-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_http2-debuginfo-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_http2-debugsource-0:1.15.7-3.module+el8.4.0+8625+d397f3da.x86_64 - mod_ldap-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_ldap-debuginfo-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_md-1:2.0.8-8.module+el8.3.0+6814+67d1e611.src - mod_md-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_md-debuginfo-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_md-debugsource-1:2.0.8-8.module+el8.3.0+6814+67d1e611.x86_64 - mod_proxy_html-1:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_proxy_html-debuginfo-1:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_session-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_session-debuginfo-0:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_ssl-1:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 - mod_ssl-debuginfo-1:2.4.37-&lt;strong&gt;41.el8&lt;/strong&gt;.x86_64 ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the module lists other packages, you can delete them.&lt;/p&gt; &lt;p&gt;Finally, regenerate the repository metadata so that it picks up the new module definition from the &lt;code&gt;modules.yaml&lt;/code&gt; file in the local directory:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# createrepo_c . Directory walk started Directory walk done - 16 packages Temporary output repo path: ./.repodata/ Preparing sqlite DBs Pool started (with 5 workers) Pool finished&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check that the module definition was imported under the known &lt;code&gt;*-module.yaml.*&lt;/code&gt; filename:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ls repodata/ 733d406732770bce66f8b790a92e933559903e7c3360a1bada3de366c130fc7c-other.sqlite.bz2 a99faa19e499bec174b3c3c682d150715fc66015da00511aa9f78691a3670708-other.xml.gz aaffc762d36b0389d602c9c37db74242cae8d37ea89d3aa3e4ca2b4cc4099b0d-primary.sqlite.bz2 d10db6feb91cc5f185218367162cdbab49780343a82efe23e6d8c0e14f4effcb-filelists.xml.gz e51d17bf9000bd130f99edd8ff2923977c8c74a0b5829116e36299fb46a440e9-primary.xml.gz f98a57f75a9fb84f1ce0313ee22e435eebb6134a28f7568ad8b8b4e14be38285-filelists.sqlite.bz2 &lt;strong&gt;ff2b17e5a515266023ccc983a8cf12401ae4d2c2049683e76ad09f6b5cea48ba-modules.yaml.gz&lt;/strong&gt; repomd.xml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can delete the &lt;code&gt;./modules.yaml&lt;/code&gt; file. You don't need it anymore. The repository is now modular.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Importing modular metadata from a &lt;code&gt;modules.yaml&lt;/code&gt; file is a new feature of &lt;code&gt;createrepo-c-0.16.2&lt;/code&gt;. If you have an older version, you need to use the &lt;code&gt;modifyrepo_c&lt;/code&gt; tool after running &lt;code&gt;createrepo_c&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Step 4: Install the package from the modular repository&lt;/h2&gt; &lt;p&gt;We are nearly done. Try updating the system again:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# yum upgrade myhttpd packages 1.4 MB/s | 26 kB 00:00 Dependencies resolved. ========================================================================================== Package Architecture Version Repository Size ========================================================================================== Upgrading: httpd x86_64 2.4.37-41.el8 myhttpd 1.4 M httpd-filesystem noarch 2.4.37-41.el8 myhttpd 37 k httpd-tools x86_64 2.4.37-41.el8 myhttpd 104 k Transaction Summary ========================================================================================== Upgrade 3 Packages Total size: 1.5 M Is this ok [y/N]: y&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And that's it. It works. Hooray!&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If YUM did not refresh the repository, it might be because you performed the steps too quickly. Clean up the cache with &lt;code&gt;rm -rf /var/cache/dnf/myhttpd*&lt;/code&gt; and try again.&lt;/p&gt; &lt;h2&gt;Step 5: Verify the patched package&lt;/h2&gt; &lt;p&gt;Finally, you can check the &lt;code&gt;Server&lt;/code&gt; header that the server returns:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ wget --no-proxy -S -O /dev/null http://localhost/ --2021-07-16 15:15:56-- http://localhost/ Resolving localhost (localhost)... ::1, 127.0.0.1 Connecting to localhost (localhost)|::1|:80... connected. HTTP request sent, awaiting response... HTTP/1.1 403 Forbidden Date: Fri, 16 Jul 2021 13:15:56 GMT &lt;strong&gt;Server: Apache/2.4.37 (My Linux)&lt;/strong&gt; Last-Modified: Mon, 12 Jul 2021 19:36:32 GMT ETag: "133f-5c6f23d09f000" Accept-Ranges: bytes Content-Length: 4927 Keep-Alive: timeout=5, max=100 Connection: Keep-Alive Content-Type: text/html; charset=UTF-8 2021-07-16 15:15:56 ERROR 403: Forbidden.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The line &lt;code&gt;Server: Apache/2.4.37 (My Linux)&lt;/code&gt; shows that the server is running your patched module.&lt;/p&gt; &lt;h2&gt;Versioning patched modules and packages&lt;/h2&gt; &lt;p&gt;What version should you use for the patched modules? Currently, it doesn't matter much because YUM merges all module versions of a single stream together. But I recommend incrementing the last digit, as we did in our example. The module version is basically a timestamp. Incrementing the last digit means moving a second ahead. It's improbable that Red Hat would release two module versions with one-second delays. Thus, when Red Hat releases a new version, it's recognized as more recent than your version, and replaces your version.&lt;/p&gt; &lt;p&gt;The version number could matter if you want to change other modular metadata, such as modular dependencies. Then the highest module version wins.&lt;/p&gt; &lt;p&gt;What about the RPM version string? Inside a stream, a standard RPM epoch-version-release comparison is used to update a modular package to another modular package. If Red Hat released a new module update, the &lt;code&gt;httpd&lt;/code&gt; package would be called something like &lt;code&gt;httpd-0:2.4.37-41.module+el8.5.0+11022+1c90597b&lt;/code&gt;. That's fine because that would be a higher RPM version string than yours and the new update would win:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ rpmdev-vercmp 0:2.4.37-41.el8 0:2.4.37-41.module+el8.5.0+11022+1c90597b 0:2.4.37-41.el8 &lt; 0:2.4.37-41.module+el8.5.0+11022+1c90597b&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you want your RPM package to win over future Red Hat updates, choose a reasonably high release number. The process is the same as what would you do in a nonmodular scenario.&lt;/p&gt; &lt;h2&gt;Special situations and warnings&lt;/h2&gt; &lt;p&gt;Sometimes there are multiple modules with the same version but a different context value. What does the context mean? Which context should you use? Can you change it?&lt;/p&gt; &lt;p&gt;The context distinguishes modules that were built from the same sources but for different environments. For instance, the &lt;code&gt;perl-DBI:1.641&lt;/code&gt; modules found in Red Hat Enterprise Linux 8.3 are built three times for three different Perl versions, so there are three different contexts of them. When you patch a module, don't change the context. Copying the old value is the safest approach.&lt;/p&gt; &lt;p&gt;I'll conclude by listing a few shortcuts that are not recommended for patching modules:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Installing from a local file. You can use &lt;code&gt;yum upgrade ./httpd-*.rpm&lt;/code&gt;, but the results won't last long. A package installed like that won't be recognized as belonging to any module and could be expelled from future YUM transactions, resulting in a dependency conflict on an RPM level. Also, having a package outside a repository makes it difficult to deploy to multiple machines or reinstall the package.&lt;/li&gt; &lt;li&gt;Adding a &lt;code&gt;module_hotfixes=true&lt;/code&gt; statement to a YUM configuration file for a nonmodular repository. While this technique works as a last resort for overriding any modular content, the hammer is too big for the nail. It does not play nicely when multiple module streams provide the same package, or if all the streams are disabled.&lt;/li&gt; &lt;li&gt;Omitting the zero epoch from an artifacts list in the module definition. Don't do it. YUM won't understand it. If your RPM package has no epoch number, write &lt;code&gt;0&lt;/code&gt;. You can use &lt;code&gt;rpm -q --qf '%{NAME}-%{EPOCHNUM}:%{VERSION}-%{RELEASE}.%{ARCH}\n' -p httpd-2.4.37-41.el8.x86_64.rpm&lt;/code&gt; to obtain the right value: &lt;code&gt;httpd-&lt;strong&gt;0:&lt;/strong&gt;2.4.37-41.el8.x86_64&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;References&lt;/h2&gt; &lt;p&gt;I recommend reading the &lt;a href="https://github.com/fedora-modularity/libmodulemd/blob/main/yaml_specs/modulemd_stream_v2.yaml"&gt;module definition format&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/03/how-patch-modules-red-hat-enterprise-linux" title="How to patch modules in Red Hat Enterprise Linux"&gt;How to patch modules in Red Hat Enterprise Linux&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rIvhVOaOwzQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Petr Pisar</dc:creator><dc:date>2021-08-03T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/03/how-patch-modules-red-hat-enterprise-linux</feedburner:origLink></entry><entry><title>What's new in the Red Hat OpenShift 4.8 console</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tM_WYj5RYNs/whats-new-red-hat-openshift-48-console" /><author><name>Serena Chechile Nichols</name></author><id>3ee6bb32-47bf-4136-b520-34b17b590fe0</id><updated>2021-08-02T07:00:00Z</updated><published>2021-08-02T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift"&gt;Red Hat OpenShift 4.8&lt;/a&gt; brings many exciting new capabilities for developers. This article focuses on what’s new in the OpenShift console, including devfiles and certified Helm charts in the developer catalog, the ability to drag and drop Spring or Quarkus JARs directly from your desktop, a streamlined developer experience for building, deploying, and scaling cloud-native applications in hybrid-cloud environments, and much more. Developers will also find enhanced features for &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Red Hat OpenShift Serverless&lt;/a&gt;, &lt;a href="https://developers.redhat.com/courses/middleware/openshift-pipelines"&gt;Red Hat OpenShift Pipelines&lt;/a&gt;, &lt;a href="https://docs.openshift.com/container-platform/4.7/cicd/gitops/understanding-openshift-gitops.html"&gt;Red Hat OpenShift GitOps&lt;/a&gt;, and more.&lt;/p&gt; &lt;h2&gt;Dragging and dropping Spring and Quarkus apps&lt;/h2&gt; &lt;p&gt;Developers can drag and drop their fat JAR files for Spring and Quarkus apps directly from their desktop into the console topology view. The system does the rest of the work to deploy the applications on OpenShift for quick and easy testing. The &lt;strong&gt;Upload JAR&lt;/strong&gt; feature lets you quickly and easily test applications before pushing them to Git, without having to build a container image. This new feature expands how you can code and test locally using a command-line interface (odo) and IDE extensions in VS Code and IntelliJ for OpenShift Connector, &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As a console user, you can still check your code into your Git repository and use the &lt;strong&gt;Import from git&lt;/strong&gt; feature in the console. You can also build a container image, push it to a registry such as &lt;a href="https://quay.io/"&gt;Quay&lt;/a&gt;, and use the &lt;strong&gt;Deploy image&lt;/strong&gt; feature from the console.&lt;/p&gt; &lt;h2&gt;Getting started&lt;/h2&gt; &lt;p&gt;To improve developer onboarding, we have a new &lt;strong&gt;Getting started resources&lt;/strong&gt; card on the &lt;strong&gt;Add&lt;/strong&gt; page, as shown in Figure 1. This card provides resources to create applications using samples, build with guided documentation, and explore new developer features.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/addpage.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/addpage.png?itok=ulhpK7Tc" width="600" height="375" alt="There is a new "Getting started resources" card on the console's Add page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The "Getting started resources" card on the Add page. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Developer catalog&lt;/h2&gt; &lt;p&gt;OpenShift 4.8 makes &lt;a href="https://www.redhat.com/en/blog/red-hat-openshift-certification-extends-support-kubernetes-native-technologies-helm"&gt;certified Helm charts&lt;/a&gt; available from the developer catalog. These charts are provided by our partners and ensure the best available integration and experience on OpenShift. A badge in the developer catalog makes it easy to identify certified Helm charts, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/helmchart.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/helmchart.png?itok=30UfzOcV" width="600" height="375" alt="Certified Helm charts from partners are now in the developer catalog." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Certified Helm charts in the developer catalog. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In many of the discussions we are having with developers, one of the biggest pain points is around configuration, settings, and managing developer environments. Developers want to advance quickly to building, running, and debugging projects. &lt;a href="https://docs.devfile.io/devfile/2.1.0/user-guide/index.html"&gt;Devfiles&lt;/a&gt; offer exactly that capability.&lt;/p&gt; &lt;p&gt;In the newest version of OpenShift, you can now find devfiles in the developer catalog, as shown in Figure 3. You can use these to bootstrap a new project for a particular language or framework. Each devfile provides sample projects for quick use.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/devfiles.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/devfiles.png?itok=jGYNIHAm" width="600" height="367" alt="Devfiles, which enable fast application setup, are available in the developer catalog." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Devfiles in the developer catalog. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Devfiles are configuration files that set up a cloud-native project with the required information to build, run, debug, and more. Devfiles are already used in odo and CodeReady Workspaces. Check out the article &lt;a href="https://developers.redhat.com/blog/2021/02/12/developing-your-own-custom-devfiles-for-odo-2-0"&gt;Developing your own custom devfiles for odo 2.0&lt;/a&gt; to learn more about creating custom devfiles.&lt;/p&gt; &lt;h2&gt;OpenShift Serverless&lt;/h2&gt; &lt;p&gt;The popular serverless method of running applications benefits from several enhancements in OpenShift 4.8.&lt;/p&gt; &lt;h3&gt;Make Serverless&lt;/h3&gt; &lt;p&gt;The &lt;strong&gt;Make Serverless&lt;/strong&gt; action, which is in tech preview, creates a new serverless deployment next to your existing deployment. Other configurations, including the traffic pattern, can be modified in the form. Figure 4 shows the new &lt;strong&gt;Make Serverless&lt;/strong&gt; action.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/serverless.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/serverless.gif" width="423" height="264" alt="This video shows how to use the Make Serverless action to create a serverless deployment." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The Make Serverless procedure. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Cloud functions&lt;/h3&gt; &lt;p&gt;The topology view in the developer console now lets you visualize cloud functions. Event sources can specify cloud functions as their sinks.&lt;/p&gt; &lt;h3&gt;Advanced scaling options for Knative services&lt;/h3&gt; &lt;p&gt;We now have enhanced scaling options for Knative services. The &lt;strong&gt;Concurrency utilization&lt;/strong&gt; feature allows you to set the percentage of concurrent requests that have to be active before scaling up. &lt;strong&gt;Autoscale window&lt;/strong&gt; allows you to set the amount of time to look back while making autoscaling decisions. The service is scaled down to zero if no requests are received in that time period.&lt;/p&gt; &lt;h2&gt;OpenShift Pipelines&lt;/h2&gt; &lt;p&gt;The OpenShift developer console now has feature parity with Tekton within the Pipeline Builder and other pipeline-related flows. The console supports &lt;a href="https://developers.redhat.com/blog/2021/02/12/developing-your-own-custom-devfiles-for-odo-2-0#"&gt;WhenExpressions&lt;/a&gt;, &lt;a href="https://github.com/tektoncd/pipeline/blob/main/docs/pipelines.md#configuring-a-pipeline"&gt;finally tasks&lt;/a&gt;, and more. &lt;code&gt;WhenExpressions&lt;/code&gt; are preceded by a diamond shape and &lt;code&gt;finally tasks&lt;/code&gt; are enclosed in a grouping with a white background, as shown in Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/whenfinally.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/whenfinally.png?itok=zFJTTKGg" width="600" height="408" alt="WhenExpressions and finally tasks can form a pipeline." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Pipeline with WhenExpressions and finally tasks. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;OpenShift GitOps&lt;/h2&gt; &lt;p&gt;With OpenShift GitOps 1.2 installed, you will see some updates to the &lt;strong&gt;Environments&lt;/strong&gt; page. You are now able to see the status of the environments to which each application has been deployed, as shown in Figure 6.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/environments.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/environments.png?itok=rpNIM1n8" width="600" height="299" alt="The Environments page shows information about each of an application's environments." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Environments page. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Managed Kafka&lt;/h2&gt; &lt;p&gt;We've provided an easy way for developers to &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka#provision_a_kafka_cluster_with_openshift_streams_for_apache_kafka_through_the_ui"&gt;create and scale event-driven applications with Apache Kafka&lt;/a&gt;. The &lt;a href="https://www.openshift.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt; Operator provides a streamlined developer experience for building, deploying, and scaling cloud-native applications in open, hybrid-cloud environments. This feature lets you connect with your application services directly in your own cluster by using the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/download"&gt;Red Hat OpenShift Application Services CLI&lt;/a&gt; and the OpenShift console UI.&lt;/p&gt; &lt;h2&gt;Customizing the developer perspective&lt;/h2&gt; &lt;p&gt;We are always looking for ways to provide a more streamlined user experience. In response to customer feedback, we now support two additional ways to customize the developer experience.&lt;/p&gt; &lt;h3&gt;Hiding features from the Add page&lt;/h3&gt; &lt;p&gt;Cluster admins can now hide features from the &lt;strong&gt;Add&lt;/strong&gt; page. To achieve this, add the &lt;code&gt;addPage&lt;/code&gt; customization in the Console spec. A code snippet is available in the YAML editor to perform this customization. That code snippet shows how to hide the &lt;strong&gt;Import from DevFile&lt;/strong&gt; entry, but you can adapt the code to your needs.&lt;/p&gt; &lt;h3&gt;Customizing roles in Project Access&lt;/h3&gt; &lt;p&gt;Cluster administrators can now customize which roles are being shown in &lt;strong&gt;Project Access&lt;/strong&gt; in the developer console. Default roles are Admin, Edit, and View. But the cluster administrator could configure a list of &lt;code&gt;ClusterRoles&lt;/code&gt; to override the default roles. To do so, add a &lt;code&gt;projectAccess&lt;/code&gt; customization in the console spec. A code snippet is available in the YAML editor to perform this customization.&lt;/p&gt; &lt;h2&gt;We want your feedback!&lt;/h2&gt; &lt;p&gt;Community feedback helps us continually improve the OpenShift developer experience, and we want to hear from you. You can attend our office hours on &lt;a href="http://openshift.tv/"&gt;Red Hat OpenShift Streaming&lt;/a&gt;, join the &lt;a href="https://groups.google.com/forum/#!forum/openshift-dev-users"&gt;OpenShift Developer Experience Google group&lt;/a&gt;, or &lt;a href="https://twitter.com/serenamarie125"&gt;tweet me your feedback directly&lt;/a&gt;. We hope you will share your tips for using the OpenShift web console, get help with what doesn’t work, and shape the future of the OpenShift developer experience. Ready to get started? &lt;a href="http://www.openshift.com/try"&gt;Try OpenShift today&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/02/whats-new-red-hat-openshift-48-console" title="What's new in the Red Hat OpenShift 4.8 console"&gt;What's new in the Red Hat OpenShift 4.8 console&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tM_WYj5RYNs" height="1" width="1" alt=""/&gt;</summary><dc:creator>Serena Chechile Nichols</dc:creator><dc:date>2021-08-02T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/02/whats-new-red-hat-openshift-48-console</feedburner:origLink></entry><entry><title type="html">Kogito 1.9.0 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/c6X4IGw2dL0/kogito-1-9-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2021/08/kogito-1-9-0-released.html</id><updated>2021-08-02T01:19:56Z</updated><content type="html">We are glad to announce that the Kogito 1.9.0 release is now available! This goes hand in hand with, . From a feature point of view, we included a series of new features and bug fixes, including: * New Task Assigning Service that enables you to automatically assign user tasks * Upgraded SpringBoot version to 2.3.10 * Adapter layer with Drools v7 API KNOWN ISSUES * Our team have identified an issue ) that currently prevents the usage of the different persistent addons on Quarkus native images. For more details head to the complete. All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found. * Kogito images are available on. * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.11.0 artifacts are available at the. A detailed changelog for 1.9.0 can be found in. New to Kogito? Check out our website. Click the "Get Started" button. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/c6X4IGw2dL0" height="1" width="1" alt=""/&gt;</content><dc:creator>Cristiano Nicolai</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/kogito-1-9-0-released.html</feedburner:origLink></entry><entry><title type="html">Bend-points and the DMN Editor</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Oixfk8_svDY/bend-points-and-the-dmn-editor.html" /><author><name>Daniel José dos Santos</name></author><id>https://blog.kie.org/2021/07/bend-points-and-the-dmn-editor.html</id><updated>2021-07-30T13:44:26Z</updated><content type="html">We’ve been working in DMN Editor to improve its user experience. We know that our users sometimes have models with many edges that overlap nodes, other edges and are hard to arrange. Even with features like the that help users on organizing the content, there are still some cases where users cannot or don’t want to split the diagram. But how can they deal with this? Users may connect the "NumC" and the "Basic" nodes, but is this the best way to organize the nodes? A simple solution is already available in the BPMN editor, and now implemented in the DMN editor relies on the use of bend-points for this kind of scenario. This feature enables users to create "flexible edges". It may be easier to reshape the edge by going around the other edges with this feature instead of rearranging the nodes, so that the edges do not overlap. PREVENTING THE PAIN The BPMN and the DMN editors rely on the same core diagram system, each of them with specific features for each use case. The beautiful thing is that when the BPMN introduced bend-points, we had in mind that it would be supported for DMN at some point too, so it was built in a way that it was decoupled from the BPMN editor, envisioning re-usability in other places than BPMN. So, on DMN, we have an Edge with a list of waypoints but only filled with two points: the source and the target node. Newcomers may think that developers from the past were doing bad coding using a list for keeping only two points, but it was made this way, keeping in mind the bend-points. So, when we finally enabled the bend-points, the DMN editor structure was ready to handle Edges of lists of waypoints. I like to point to this case and show how we, as developers, may keep our code ready for expansions, especially if it is a new feature that we already have on our radar like this one. It saves time for people from the future who maybe are ourselves. The support for bend-points will be available in the next Kogito release. Stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Oixfk8_svDY" height="1" width="1" alt=""/&gt;</content><dc:creator>Daniel José dos Santos</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/bend-points-and-the-dmn-editor.html</feedburner:origLink></entry><entry><title>Avoiding dual writes in event-driven applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aN8toxH78qM/avoiding-dual-writes-event-driven-applications" /><author><name>Bernard Tison</name></author><id>41c4e1a8-fdd0-49fb-9b56-053be5d91b86</id><updated>2021-07-30T07:00:00Z</updated><published>2021-07-30T07:00:00Z</published><summary type="html">&lt;p&gt;Dual writes frequently cause issues in distributed, event-driven applications. A &lt;em&gt;dual write&lt;/em&gt; occurs when an application has to change data in two different systems, such as when an application needs to persist data in the database and send a Kafka message to notify other systems. If one of these two operations fails, you might end up with inconsistent data. Dual writes can be hard to detect and fix.&lt;/p&gt; &lt;p&gt;In this article, you will learn how to use the outbox pattern with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; to avoid the dual write problem in event-driven applications. I will show you how to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Provision a Kafka cluster on OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Deploy and configure Debezium to use OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Run an application that uses Debezium and OpenShift Streams for Apache Kafka to implement the outbox pattern.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;OpenShift Streams for Apache Kafka&lt;/a&gt; is an Apache Kafka service that is fully hosted and managed by Red Hat. The service is useful for developers who want to incorporate streaming data and scalable messaging in their applications without the burden of setting up and maintaining a Kafka cluster infrastructure.&lt;/p&gt; &lt;p&gt;&lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; is an open source, distributed platform for change data capture. Built on top of Apache Kafka, Debezium allows applications to react to inserts, updates, and deletes in your databases.&lt;/p&gt; &lt;h2&gt;Demo: Dual writes in an event-driven system&lt;/h2&gt; &lt;p&gt;The demo application we'll use in this article is part of a distributed, event-driven order management system. The application suffers from the dual write issue: When a new order comes in through a REST interface, the order is persisted in the database—in this case PostgreSQL—and an &lt;code&gt;OrderCreated&lt;/code&gt; event is sent to a Kafka topic. From there, it can be consumed by other parts of the system.&lt;/p&gt; &lt;p&gt;You will find the code for the order service application in this &lt;a href="https://github.com/rhosak-debezium-outbox/order-service"&gt;Github repository&lt;/a&gt;. The application was developed using &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;. The structure of the &lt;code&gt;OrderCreated&lt;/code&gt; events follows the &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt; specification, which defines a common way to describe event data.&lt;/p&gt; &lt;h3&gt;Solving dual writes with the outbox pattern&lt;/h3&gt; &lt;p&gt;Figure 1 shows the architecture of the outbox pattern implemented with Debezium and OpenShift Streams for Apache Kafka. For this example, you will also use Docker to spin up the different application components on your local system.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/arch.png?itok=LtcRgFRW" width="600" height="338" alt="In the outbox pattern with Apache Kafka, Debezium monitors inserts and informs Kafka, which in turn tells the event consumers about the change." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Architecture of the outbox pattern with Debezium and OpenShift Streams for Apache Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: For a thorough discussion of Debezium and the outbox pattern see &lt;a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/"&gt;Reliable Microservices Data Exchange With the Outbox Pattern&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Prerequisites for the demonstration&lt;/h3&gt; &lt;p&gt;This article assumes that you already have an OpenShift Streams for Apache Kafka instance in your development environment. Visit the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; page to create a Kafka instance. See the article &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; for the basics of creating Kafka instances, topics, and service accounts.&lt;/p&gt; &lt;p&gt;I also assume that you have Docker installed on your local system.&lt;/p&gt; &lt;h2&gt;Provision a Kafka cluster with OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;After you set up an OpenShift Streams for Apache Kafka instance, you will create environment variables for the Kafka bootstrap server endpoint and the service account credentials. Use the following environment variables when configuring the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export KAFKA_BOOTSTRAP_SERVER=&lt;value of the Bootstrap server endpoint&gt; $ export CLIENT_ID=&lt;value of the service account Client ID&gt; $ export CLIENT_SECRET=&lt;value of the service account Client Secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a topic named &lt;code&gt;order-event&lt;/code&gt; on your Kafka instance for the user service application, as shown in Figure 2. The number of partitions is not critical for this example. I generally use 15 partitions for Kafka topics as a default. You can leave the message retention time at seven days.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topic.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/topic.png?itok=5CDjuuoC" width="600" height="392" alt="The Kafka instance for the outbox pattern shows information about the order-event." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The order-event Kafka topic on OpenShift Streams for Apache Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Persisting the order service&lt;/h3&gt; &lt;p&gt;The order service application exposes a REST endpoint for new orders. When a new order is received, the order is persisted using JPA in the &lt;code&gt;orders&lt;/code&gt; table of the PostgreSQL database. In the same transaction, the outbox event for the &lt;code&gt;OrderCreated&lt;/code&gt; message is written to the &lt;code&gt;orders_outbox&lt;/code&gt; table. See &lt;a href="https://github.com/rhosak-debezium-outbox/order-service"&gt;this Github repo&lt;/a&gt; for the application source code. The &lt;code&gt;OrderService&lt;/code&gt; class contains the code for persisting the order entity and the &lt;code&gt;OrderCreated&lt;/code&gt; message:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@ApplicationScoped public class OrderService { @Inject EntityManager entityManager; @ConfigProperty(name = "order-event.aggregate.type", defaultValue = "order-event") String aggregateType; @Transactional public Long create(Order order) { order.setStatus(OrderStatus.CREATED); entityManager.persist(order); OutboxEvent outboxEvent = buildOutBoxEvent(order); entityManager.persist(outboxEvent); entityManager.remove(outboxEvent); return order.getId(); } OutboxEvent buildOutBoxEvent(Order order) { OutboxEvent outboxEvent = new OutboxEvent(); outboxEvent.setAggregateType(aggregateType); outboxEvent.setAggregateId(Long.toString(order.getId())); outboxEvent.setContentType("application/cloudevents+json; charset=UTF-8"); outboxEvent.setPayload(toCloudEvent(order)); return outboxEvent; } String toCloudEvent(Order order) { CloudEvent event = CloudEventBuilder.v1().withType("OrderCreatedEvent") .withTime(OffsetDateTime.now()) .withSource(URI.create("ecommerce/order-service")) .withDataContentType("application/json") .withId(UUID.randomUUID().toString()) .withData(order.toJson().encode().getBytes()) .build(); EventFormat format = EventFormatProvider.getInstance() .resolveFormat(JsonFormat.CONTENT_TYPE); return new String(format.serialize(event)); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The structure of the &lt;code&gt;OrderCreated&lt;/code&gt; message follows the CloudEvents specification. The code uses the Java SDK for CloudEvents API to build the CloudEvent and serialize it to JSON format.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;ContentType&lt;/code&gt; field of the &lt;code&gt;OutboxEvent&lt;/code&gt; entity is set to &lt;code&gt;application/cloudevents+json&lt;/code&gt;. When processed by a Debezium single message transformation (SMT), this value is set as the &lt;code&gt;content-type&lt;/code&gt; header on the Kafka message, as mandated by the CloudEvents specification.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;I'll discuss the structure of the Outbox Event table shortly. The &lt;code&gt;OutboxEvent&lt;/code&gt; entity is persisted to the database and then removed right away. Debezium, being log-based, does not examine the contents of the database table; it just tails the append-only transaction log. The code will generate an &lt;code&gt;INSERT&lt;/code&gt; and a &lt;code&gt;DELETE&lt;/code&gt; entry in the log when the transaction commits. Debezium processes both events, and produces a Kafka message for any &lt;code&gt;INSERT&lt;/code&gt;. However, &lt;code&gt;DELETE&lt;/code&gt; events are ignored.&lt;/p&gt; &lt;p&gt;The net result is that Debezium is able to capture the event added to the outbox table, but the table itself remains empty. No additional disk space is needed for the table and no separate housekeeping process is required to stop it from growing indefinitely.&lt;/p&gt; &lt;h3&gt;Running the PostgreSQL database&lt;/h3&gt; &lt;p&gt;Run the PostgreSQL database as a Docker container. The database and the &lt;code&gt;orders&lt;/code&gt; and &lt;code&gt;orders_outbox&lt;/code&gt; tables are created when the container starts up:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -d --name postgresql \ -e POSTGRESQL_USER=orders -e POSTGRESQL_PASSWORD=orders \ -e POSTGRESQL_ADMIN_PASSWORD=admin -e POSTGRESQL_DATABASE=orders \ quay.io/btison_rhosak/postgresql-order-service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, spin up the container for the order service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -d --name order-service -p 8080:8080 \ --link postgresql -e DATABASE_USER=orders \ -e DATABASE_PASSWORD=orders -e DATABASE_NAME=orders \ -e DATABASE_HOST=postgresql -e ORDER_EVENT_AGGREGATE_TYPE=order-event \ quay.io/btison_rhosak/order-service-outbox&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configure and run Debezium&lt;/h2&gt; &lt;p&gt;Debezium is implemented as a &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Kafka Connect&lt;/a&gt; connector, so the first thing to do is to spin up a Kafka Connect container, pointing to the managed Kafka instance.&lt;/p&gt; &lt;p&gt;The Kafka Connect image we use here is derived from the Kafka Connect image provided by the &lt;a href="https://strimzi.io"&gt;Strimzi&lt;/a&gt; project. The Debezium libraries and the Debezium PostgreSQL connector are already installed on this image.&lt;/p&gt; &lt;p&gt;Kafka Connect is configured using a properties file, which specifies among other things how Kafka Connect should connect to the Kafka broker.&lt;/p&gt; &lt;p&gt;To connect to the managed Kafka instance, this application employs &lt;a href="https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_plain.html"&gt;SASL/PLAIN&lt;/a&gt; authentication over TLS, using the client ID and secret from the service account you created earlier as credentials.&lt;/p&gt; &lt;h3&gt;Creating a configuration file&lt;/h3&gt; &lt;p&gt;Create a configuration file on your local file system. The file refers to environment variables for the Kafka bootstrap address and the service account credentials:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ cat &lt;&lt; EOF &gt; /tmp/kafka-connect.properties # Bootstrap servers bootstrap.servers=$KAFKA_BOOTSTRAP_SERVER # REST Listeners rest.port=8083 # Plugins plugin.path=/opt/kafka/plugins # Provided configuration offset.storage.topic=kafka-connect-offsets value.converter=org.apache.kafka.connect.json.JsonConverter config.storage.topic=kafka-connect-configs key.converter=org.apache.kafka.connect.json.JsonConverter group.id=kafka-connect status.storage.topic=kafka-connect-status config.storage.replication.factor=3 key.converter.schemas.enable=false offset.storage.replication.factor=3 status.storage.replication.factor=3 value.converter.schemas.enable=false security.protocol=SASL_SSL producer.security.protocol=SASL_SSL consumer.security.protocol=SASL_SSL admin.security.protocol=SASL_SSL sasl.mechanism=PLAIN producer.sasl.mechanism=PLAIN consumer.sasl.mechanism=PLAIN admin.sasl.mechanism=PLAIN sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; producer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; admin.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; EOF&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Launching the Kafka Connect container&lt;/h3&gt; &lt;p&gt;Next, you will launch the Kafka Connect container. The properties file you just created is mounted into the container. The container is also linked to the PostgreSQL container, so that the Debezium connector can connect to PostgreSQL to access the transaction logs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -d --name kafka-connect --link postgresql -p 8083:8083 \ --mount type=bind,source=/tmp/kafka-connect.properties,destination=/config/kafka-connect.properties \ quay.io/btison_rhosak/kafka-connect-dbz-pgsql:1.7.0-1.5.0.Final&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now check the Kafka Connect container logs. Kafka Connect logs are quite verbose, so if you don’t see any stack traces, you can assume Kafka Connect is running fine and successfully connected to the Kafka cluster.&lt;/p&gt; &lt;h3&gt;Configuring the Debezium connector&lt;/h3&gt; &lt;p&gt;Kafka Connect exposes a REST endpoint through which you can deploy and manage Kafka Connect connectors, such as the Debezium connector. Create a file on your local file system for the Debezium connector configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ cat &lt;&lt; EOF &gt; /tmp/debezium-connector.json { "name": "debezium-postgres-orders", "config": { "connector.class": "io.debezium.connector.postgresql.PostgresConnector", "plugin.name": "pgoutput", "database.hostname": "postgresql", "database.port": "5432", "database.user": "postgres", "database.password": "admin", "database.dbname": "orders", "database.server.name": "orders1", "schema.whitelist": "public", "table.whitelist": "public.orders_outbox", "tombstones.on.delete" : "false", "transforms": "router", "transforms.router.type": "io.debezium.transforms.outbox.EventRouter", "transforms.router.table.fields.additional.placement": "content_type:header:content-type", "transforms.router.route.topic.replacement": "\${routedByValue}" } } EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's look more closely at some of the fields in this configuration:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;plugin-name&lt;/strong&gt;: The Debezium connector uses a PostgreSQL output plug-in to extract changes committed to the transaction log. In this case, we use the &lt;code&gt;pgoutput&lt;/code&gt; plug-in, which is included in PostgreSQL since version 10. See the &lt;a href="https://debezium.io/documentation/reference/1.5/connectors/postgresql.html"&gt;Debezium documentation&lt;/a&gt; for information about output plug-ins.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;database.*&lt;/strong&gt;: These settings allow Debezium to connect to the PostgreSQL database. This example specifies the &lt;code&gt;postgres&lt;/code&gt; system user, which has superuser privileges. In a production system, you should probably create a dedicated Debezium user with the necessary privileges.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;table.whitelist&lt;/strong&gt;: This specifies the list of tables that are monitored for changes by the Debezium Connector.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;tombstones.on.delete&lt;/strong&gt;: This indicates whether a deletion marker ("tombstones") should be emitted by the connector when a record is deleted from the outbox table. By setting &lt;code&gt;tombstones.on.delete&lt;/code&gt; to false, you tell Debezium to effectively ignore deletes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;transforms.*&lt;/strong&gt;: These settings describe how Debezium should process database change events. Debezium applies a single message transform (SMT) to every captured change event. For the outbox pattern, Debezium uses the built-in &lt;code&gt;EventRouter&lt;/code&gt; SMT, which extracts the new state of the change event, transforms it into a Kafka message, and sends it to the appropriate topic.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;More about the EventRouter&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;EventRouter&lt;/code&gt; by default makes certain assumptions about the structure of the outbox table:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; Column | Type | Modifiers --------------+------------------------+----------- id | uuid | not null aggregatetype | character varying(255) | not null aggregateid | character varying(255) | not null payload | text | not null content_type | character varying(255) | not null &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;EventRouter&lt;/code&gt; calculates the value of the destination topic from the &lt;code&gt;aggregatetype&lt;/code&gt; column and the value of the &lt;code&gt;route.topic.replacement&lt;/code&gt; configuration (where &lt;code&gt;${routedBy}&lt;/code&gt; represents the value in the &lt;code&gt;aggregatetype&lt;/code&gt; column). The key of the Kafka message is the value of the &lt;code&gt;aggregateid&lt;/code&gt; column, and the payload is whatever is in the &lt;code&gt;payload&lt;/code&gt; column. The &lt;code&gt;table.fields.additional.placement&lt;/code&gt; parameter defines how additional columns should be handled. In our case, we specify that the value of the &lt;code&gt;content_type&lt;/code&gt; column should be added to the Kafka message as a header with key &lt;code&gt;content-type&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Deploying the Debezium connector&lt;/h3&gt; &lt;p&gt;Deploy the Debezium connector by calling the Kafka Connect REST endpoint:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X POST -H "Accept: application/json" -H "Content-type: application/json" \ -d @/tmp/debezium-connector.json 'http://localhost:8083/connectors'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the logs of the &lt;code&gt;kafka-connect&lt;/code&gt; container to verify that the Debezium connector was installed successfully. If you were successful, you’ll see something like the following toward the end of the logs:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;2021-06-09 21:09:46,944 INFO user 'postgres' connected to database 'orders' on PostgreSQL 12.5 on x86_64-redhat-linux-gnu, compiled by gcc (GCC) 8.3.1 20191121 (Red Hat 8.3.1-5), 64-bit with roles: role 'pg_read_all_settings' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_stat_scan_tables' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_write_server_files' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_monitor' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_read_server_files' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'orders' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: true] role 'pg_execute_server_program' [superuser: false, replication: false, inherit: true, create role: false, create db: false,can log in: false] role 'pg_read_all_stats' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can login: false] role 'pg_signal_backend' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can login: false] role 'postgres' [superuser: true, replication: true, inherit: true, create role: true, create db: true, can log in: true] (io.debezium.connector.postgresql.PostgresConnectorTask) [task-thread-debezium-postgres-orders-0] &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Consume the Kafka messages&lt;/h2&gt; &lt;p&gt;Next, you want to view the Kafka messages produced by Debezium and sent to the &lt;code&gt;order-event&lt;/code&gt; topic. You can use a tool such as &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt; to process the messages. The following &lt;code&gt;docker&lt;/code&gt; command launches a container hosting the &lt;code&gt;kafkacat&lt;/code&gt; utility and consumes all the messages in the &lt;code&gt;order-event&lt;/code&gt; topic from the beginning:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -it --rm edenhill/kafkacat:1.6.0 kafkacat \ -b $KAFKA_BOOTSTRAP_SERVER -t order-event \ -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN \ -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" \ -f 'Partition: %p\n Key: %k\n Headers: %h\n Payload: %s\n' -C&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Test the application&lt;/h2&gt; &lt;p&gt;All components are in place to test the order service application. To use the REST interface to create an order, issue the following cURL command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -v -X POST -H "Content-type: application/json" \ -d '{"customerId": "customer123", "productCode": "XXX-YYY", "quantity": 3, "price": 159.99}' \ http://localhost:8080/order&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Verify with &lt;code&gt;kafkacat&lt;/code&gt; that a Kafka message has been produced to the &lt;code&gt;order-event&lt;/code&gt; topic. When you issue the &lt;code&gt;kafkacat&lt;/code&gt; command mentioned earlier, the output should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Partition: 3 Key: "992" Headers: id=743e3736-f9e3-4c2f-bce7-eaa35afe8876,content-type=application/cloudevents+json; charset=UTF-8 Payload: "{\"specversion\":\"1.0\",\"id\":\"843d8770-f23d-41e2-a697-a64367f1d387\",\"source\":\"ecommerce/order-service\",\"type\":\"OrderCreatedEvent\",\"datacontenttype\":\"application/json\",\"time\":\"2021-06-10T07:40:52.282602Z\",\"data\":{\"id\":992,\"customerId\":\"customer123\",\"productCode\":\"XXX-YYY\",\"quantity\":3,\"price\":159.99,\"status\":\"CREATED\"}}" % Reached end of topic order-event [3] at offset 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the ID of the outbox event is added as a header to the message. This information can be exploited by consumers for duplicate detection. The &lt;code&gt;content-type&lt;/code&gt; header is added by the Debezium &lt;code&gt;EventRouter&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Great job! If you’ve followed along, you have successfully:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Provisioned a &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;managed Kafka&lt;/a&gt; instance on &lt;a href="https://cloud.redhat.com"&gt;cloud.redhat.com&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Used &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Kafka Connect&lt;/a&gt; and &lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; to connect to the managed Kafka instance.&lt;/li&gt; &lt;li&gt;Implemented and tested the outbox pattern with Debezium.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Setting up and maintaining a Kafka cluster can be tedious and complex. OpenShift Streams for Apache Kafka takes away that burden, allowing you to focus on implementing services and business logic.&lt;/p&gt; &lt;p&gt;Applications can connect to the managed Kafka instance from everywhere, so it doesn’t really matter where these applications run, whether it's on a private or public cloud, or even in Docker containers on your local workstation.&lt;/p&gt; &lt;p&gt;Stay tuned for more articles on interesting use cases and demos with OpenShift Streams for Apache Kafka.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications" title="Avoiding dual writes in event-driven applications"&gt;Avoiding dual writes in event-driven applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aN8toxH78qM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Bernard Tison</dc:creator><dc:date>2021-07-30T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications</feedburner:origLink></entry><entry><title type="html">Why some prometheus alerts in k8s can confuse</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FDn2otUDpuU/" /><author><name /></author><id>https://blog.ramon-gordillo.dev/2021/07/why-some-prometheus-alerts-in-k8s-can-confuse/</id><updated>2021-07-30T00:00:00Z</updated><content type="html">Recently I was installing a kubernetes cluster as I usually do for my tests. However, as those machines were bare metal servers that some colleagues have recycled, we decided to keep it running and try to maintain it by ourselves. First thing I did was a simple bot to send the alerts to a telegram channel. That was something I did not do in the past, because I do not care about monitoring as my clusters were ephemeral. After two days, an alert started firing. This blog is my analysis of what this alert means and why I consider it not accurate, so I ended silencing it (but I would love to have an accurate alert for that situation). THE ALERT The alert in question is KubeMemoryOvercommit. It is defined in this , and its description says: Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure. The rule expression for this alert is sum(namespace_memory:kube_pod_container_resource_requests:sum{}) / sum(kube_node_status_allocatable{resource="memory"}) &gt; ((count(kube_node_status_allocatable{resource="memory"}) &gt; 1) - 1) / count(kube_node_status_allocatable{resource="memory"}) Let’s try to understand what it means. THE CALCULATIONS Looking at the different members of the equation, we can see what it means: sum(namespace_memory:kube_pod_container_resource_requests:sum{}) = sum of memory requests in bytes for all pods in every namespace sum(kube_node_status_allocatable{resource="memory"}) = sum of memory that can be allocated to pods in every nodeernel Dividing both, we obtain the ratio of memory allocated related with the total amount that can be. Let’s go for the second part. count(kube_node_status_allocatable{resource="memory"}) = number of nodes that can be used to deploy pods ((count(kube_node_status_allocatable{resource="memory"}) &gt; 1) - 1) = number of nodes that can be used to deploy pods minus one, which should be at least one = remaining nodes which can allocate pods in case one is down Dividing the latter by the former, we got a ratio of (nodes - 1)/nodes Having the details in mind, a summary of this equation is It seems very easy to understand, but it is accurate? Let’s see what it is not in the formula. WHAT IS NOT CONSIDERED? I will try to summarize some of my thoughts in bullets. For the first part, to know if pods of a node can be reallocatable or not, there are lots of different concepts to bear in mind: * Daemonsets deploys pods per node, and this pod is meant for that node. It cannot be moved. * NodeSelector can be used to restrict which nodes a pod can land into. * Affinity/anti-affinity rules can also restrict where pods can be deployed * Taints and tolerations can avoid pods to be scheduled in those nodes. * Special hardware requirements, for example GPUs, are also restricted to some nodes of the cluster. Rules for deploying use to be based on For the second part of the equation: * Nodes can be different in size, so the second part of the equation is not accurate either. For example, if we have 1 nodes of 512 Gb RAM and 1 of 64 Gb RAM, if you have one half full and one empty (which you can see it will not fire the alert), if the full node is the bigger one and it goes down, you cannot reallocate the pods in the smaller, but it will be possible on the opposite direction. CONCLUSION I find it very useful to know if the platform has any risk of overcommitting the workloads if a node shuts down (either abruptly or planned). I find it particularly relevant in upgrades, where nodes usually have to be rebooted during the upgrade in a rolling process. However, as I have shown previously, there are so many factors to consider than a simple rule based on a couple of metrics gives a false security feeling. The only “accurate” solution that comes to my mind is if we can tell the scheduler to simulate the drain of a node and throw any issues that could be foreseen, like an extension of what is with kubectl drain --dry-run&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FDn2otUDpuU" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://blog.ramon-gordillo.dev/2021/07/why-some-prometheus-alerts-in-k8s-can-confuse/</feedburner:origLink></entry><entry><title>Troubleshooting application performance with Red Hat OpenShift metrics, Part 4: Gathering performance metrics</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3oYKYRb3wLU/troubleshooting-application-performance-red-hat-openshift-metrics-part-4" /><author><name>Pavel Macik</name></author><id>66873ac1-84f5-4f2b-83ad-fb28e2a5b5a7</id><updated>2021-07-29T07:00:00Z</updated><published>2021-07-29T07:00:00Z</published><summary type="html">&lt;p&gt;This series shows how to use &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; metrics in a real-life performance testing scenario. I used these metrics to run performance tests on the &lt;a href="https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator"&gt;Service Binding Operator&lt;/a&gt;. We used the results to performance-tune the Service Binding Operator for acceptance into the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3"&gt;Part 3&lt;/a&gt;, I showed you how we set up &lt;a href="https://docs.openshift.com/container-platform/4.7/monitoring/understanding-the-monitoring-stack.html?extIdCarryOver=true&amp;intcmp=7013a000002w3nnAAA&amp;sc_cid=7013a000002w0ZmAAI"&gt;OpenShift's monitoring stack&lt;/a&gt; to collect runtime metrics for our testing scenarios. I also shared a collector script that ensures the results are preserved on a node that won't crash. Now, we can look at the performance metrics we'll use and how to gather the data we need.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the whole series&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1"&gt;Performance requirements&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2021/07/15/troubleshooting-application-performance-red-hat-openshift-metrics-part-2-test"&gt;The test environment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3"&gt;Collecting runtime metrics&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Part 4: Gathering performance metrics&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Part 5: Test rounds and results (August 5)&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;CPU and memory usage&lt;/h2&gt; &lt;p&gt;As mentioned in &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3"&gt;Part 3&lt;/a&gt;, some metrics have to be collected for the duration of the test. The most important metrics required by the Developer Sandbox team were the CPU and memory usage of both OpenShift nodes and the tested operators. We also wanted to see the number of resources left in the cluster after the test was done.&lt;/p&gt; &lt;p&gt;Unfortunately, we ran into problems during our first attempts to run the stress tests. The OpenShift cluster actually crashed when one of its worker nodes came down. Naturally, it was important to know what caused the failure. I needed a more granular view of the CPU and memory usage, so I had to collect data from the workloads deployed on those nodes.&lt;/p&gt; &lt;p&gt;From watching and inspection using the cluster's own &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; instance, I identified a couple of resources that were loaded and stressed by our scenario. Then, I included them to be watched by the &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3#collecting_runtime_metrics_with_the_openshift_monitoring_tool"&gt;collector script&lt;/a&gt; I shared in the previous article, together with the cluster's nodes.&lt;/p&gt; &lt;p&gt;I identified workloads from a handful of namespaces as stressed, and included them to be watched for CPU and memory usage. Table 1 shows these namespaces and workloads.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 1: Workloads to watch for CPU and memory usage.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;th scope="col"&gt;Namespace&lt;/th&gt; &lt;th scope="col"&gt;Workloads&lt;/th&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-apiserver&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;apiserver-*&lt;/code&gt; pods&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-kube-apiserver&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;kube-apiserver-*&lt;/code&gt; pods&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-monitoring&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;prometheus-k8s-*&lt;/code&gt; pods&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-operators&lt;/code&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;service-binding-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;li&gt;&lt;code&gt;rhoas-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-operator-lifecycle-manager&lt;/code&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;catalog-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;li&gt;&lt;code&gt;olm-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;Resources created in the cluster&lt;/h2&gt; &lt;p&gt;Another one of the metrics requested by the Developer Sandbox team was the number of resources created in the cluster during the test. There were two ways to get that information:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;If the Prometheus instance was available (meaning the node on which it is deployed had not crashed), I could use a simple Prometheus query: &lt;pre&gt; &lt;code&gt;sort_desc(cluster:usage:resources:sum)&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;If the Prometheus instance was not available, I had to employ a brute-force approach, using the &lt;code&gt;oc&lt;/code&gt; tool to count the number of each resource.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Service Binding Operator performance and developer experience&lt;/h2&gt; &lt;p&gt;The final set of metrics was related to the performance of the Service Binding Operator itself, from a developer's perspective. Specifically, we wanted to know how long it took to perform the binding after the &lt;code&gt;ServiceBinding&lt;/code&gt; resource was created.&lt;/p&gt; &lt;p&gt;The typical situation for a developer using the Service Binding Operator is to have a backing service and an application running that the user wants to bind together. So, the developer sends a &lt;code&gt;ServiceBinding&lt;/code&gt; request and expects the binding to be done by Service Binding Operator. The scenario can be split into the following sequence of steps. Each step is shown along with the way to retrieve the respective timestamp:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;The &lt;code&gt;ServiceBinding&lt;/code&gt; request is sent, processed by OpenShift, and created internally as a resource (&lt;code&gt;.metadata.creationTimestamp&lt;/code&gt; of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource).&lt;/li&gt; &lt;li&gt;The Service Binding Operator picks up the resource while watching for it and processes it. (This is the first "Reconciling ServiceBinding" message for the particular &lt;code&gt;ServiceBinding&lt;/code&gt; resource in the Service Binding Operator logs.)&lt;/li&gt; &lt;li&gt;Based on the content of the resource, the Service Binding Operator performs the binding. It collects bindable information from the backing service and injects it into the application.&lt;/li&gt; &lt;li&gt;The application is re-deployed with the bound information injected into the &lt;code&gt;Deployment&lt;/code&gt; resource (&lt;code&gt;.status.conditions[] | select(.type=="Available") | select(.status=="True").lastTransitionTime&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;The &lt;code&gt;ServiceBinding&lt;/code&gt; resource is marked as done. (A "Done" message is sent for the particular &lt;code&gt;ServiceBinding&lt;/code&gt; resource in the Service Binding Operator logs.)&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;I defined the following metrics to evaluate the developer experience in this scenario:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Time to Ready&lt;/strong&gt;: The time between the creation of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource (1) and completion of the binding (5). That can be further split into the following: &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Time to Reconcile&lt;/strong&gt;: The time between the creation of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource (1) and when it is picked up by Service Binding Operator (2).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reconcile to Ready&lt;/strong&gt;: The time between when the Service Binding Operator picks up the &lt;code&gt;ServiceBinding&lt;/code&gt; (2) and completes the binding (5).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;At the time of the performance evaluation, these metrics were not collected by OpenShift's monitoring stack or by the Service Binding Operator. So, I had to dig up the information from the data that I could get. As shown earlier, I derived the information I needed from metadata that OpenShift collects about the active user's backing service and the application (especially timestamps), along with the Service Binding Operator logs. I wrote the following script to collect the necessary data from OpenShift and compute the information after the test is complete:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;if [ -z "$QUAY_NAMESPACE" ]; then echo "QUAY_NAMESPACE environemnt variable needs to be set to a non-empty value" exit 1 fi DT=$(date "+%F_%T") RESULTS=results-$DT mkdir -p $RESULTS USER_NS_PREFIX=${1:-zippy} # Resource counts resource_counts(){ echo -n "$1;" # All resource counts from user namespaces echo -n "$(oc get $1 --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace | grep $USER_NS_PREFIX | wc -l)" echo -n ";" # All resource counts from all namespaces echo "$(oc get $1 --all-namespaces -o name | wc -l)" } # Dig various timestamps out timestamps(){ SBR_JSON=$1 DEPLOYMENTS_JSON=$2 SBO_LOG=$3 RESULTS=$4 jq -rc '((.metadata.namespace) + ";" + (.metadata.name) + ";" + (.metadata.creationTimestamp) + ";" + (.status.conditions[] | select(.type=="Ready").lastTransitionTime))' $SBR_JSON &gt; $RESULTS/tmp.csv echo "ServiceBinding;Created;ReconciledTimestamp;Ready;AllDoneTimestamp" &gt; $RESULTS/sbr-timestamps.csv for i in $(cat $RESULTS/tmp.csv); do ns=$(echo -n $i | cut -d ";" -f1) name=$(echo -n $i | cut -d ";" -f2) echo -n $ns/$name; echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f3) "+%F %T"); echo -n ";"; log=$(cat $SBO_LOG | grep $ns) date -d @$(echo $log | jq -rc 'select(.msg | contains("Reconciling")).ts' | head -n1) "+%F %T.%N" | tr -d "\n" echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f4) "+%F %T"); echo -n ";"; done_ts=$(echo $log | jq -rc 'select(.msg | contains("Done")) | select(.retry==false).ts') if [ -n "$done_ts" ]; then date -d "@$done_ts" "+%F %T.%N" else echo "" fi done &gt;&gt; $RESULTS/sbr-timestamps.csv rm -f $RESULTS/tmp.csv jq -rc '((.metadata.namespace) + ";" + (.metadata.name) + ";" + (.metadata.creationTimestamp) + ";" + (.status.conditions[] | select(.type=="Available") | select(.status=="True").lastTransitionTime)) + ";" + (.metadata.managedFields[] | select(.manager=="manager").time)' $DEPLOYMENTS_JSON &gt; $RESULTS/tmp.csv echo "Namespace;Deployment;Deployment_Created;Deployment_Available;Deployment_Updated_by_SBO;SB_Name;SB_created;SB_ReconciledTimestamp;SB_Ready;SB_AllDoneTimestamp" &gt; $RESULTS/binding-timestamps.csv for i in $(cat $RESULTS/tmp.csv); do NS=$(echo -n $i | cut -d ";" -f1); echo -n $NS; echo -n ";"; echo -n $(echo -n $i | cut -d ";" -f2); echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f3) "+%F %T"); echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f4) "+%F %T"); echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f5) "+%F %T"); echo -n ";"; cat $RESULTS/sbr-timestamps.csv | grep $NS done &gt;&gt; $RESULTS/binding-timestamps.csv rm -f $RESULTS/tmp.csv } # Collect timestamps { # ServiceBinding resources in user namespaces oc get sbr --all-namespaces -o json | jq -r '.items[] | select(.metadata.namespace | contains("'$USER_NS_PREFIX'"))' &gt; $RESULTS/service-bindings.json # Deployment resources in user namespaces oc get deploy --all-namespaces -o json | jq -r '.items[] | select(.metadata.namespace | contains("'$USER_NS_PREFIX'"))' &gt; $RESULTS/deployments.json # ServiceBiding operator log oc logs $(oc get $(oc get pods -n openshift-operators -o name | grep service-binding-operator) -n openshift-operators -o jsonpath='{.metadata.name}') -n openshift-operators &gt; $RESULTS/service-binding-operator.log timestamps $RESULTS/service-bindings.json $RESULTS/deployments.json $RESULTS/service-binding-operator.log $RESULTS } &amp; # Collect resource counts { RESOURCE_COUNTS_OUT=$RESULTS/resource-count.csv echo "Resource;UserNamespaces;AllNamespaces" &gt; $RESOURCE_COUNTS_OUT for i in $(cat resources.list); do resource_counts $i &gt;&gt; $RESOURCE_COUNTS_OUT; done } &amp; wait &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;This article introduced the metrics we collected to performance-test the Service Binding Operator for acceptance into the Developer Sandbox for Red Hat OpenShift. I showed how we collected both metrics of interest to the Developer Sandbox team, and additional metrics for our team specifically. Look for Part 5, the final article in this series, where I will present the testing rounds and their results.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/29/troubleshooting-application-performance-red-hat-openshift-metrics-part-4" title="Troubleshooting application performance with Red Hat OpenShift metrics, Part 4: Gathering performance metrics"&gt;Troubleshooting application performance with Red Hat OpenShift metrics, Part 4: Gathering performance metrics&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3oYKYRb3wLU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Pavel Macik</dc:creator><dc:date>2021-07-29T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/29/troubleshooting-application-performance-red-hat-openshift-metrics-part-4</feedburner:origLink></entry><entry><title type="html">Separating RESTEasy Spring And Microprofile Components Into Independent Subprojects.</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/o1oznQA48yA/" /><author><name /></author><id>https://resteasy.github.io/2021/07/29/separate-spring-and-microprofile/</id><updated>2021-07-29T00:00:00Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/o1oznQA48yA" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/07/29/separate-spring-and-microprofile/</feedburner:origLink></entry><entry><title>Write toolchain-agnostic RPM spec files for GCC and Clang</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6Vh3_fS-NoA/write-toolchain-agnostic-rpm-spec-files-gcc-and-clang" /><author><name>Timm Baeder</name></author><id>646d4267-7a40-4821-969a-2740bffb8c0e</id><updated>2021-07-28T07:00:00Z</updated><published>2021-07-28T07:00:00Z</published><summary type="html">&lt;p&gt;With the addition of the &lt;code&gt;%toolchain&lt;/code&gt; macro to the &lt;code&gt;redhat-rpm-config&lt;/code&gt; package, packages can easily switch between the &lt;a href="https://gcc.gnu.org/"&gt;GNU Compiler Collection&lt;/a&gt; (GCC) and the &lt;a href="https://clang.llvm.org/"&gt;Clang compiler&lt;/a&gt;. This package change is not yet supported by Fedora, and package maintainers need good reasons to switch from the GCC default to Clang. Maintainers also need to watch out for a few nuances to make (and keep) a package specification file buildable with both toolchains.&lt;/p&gt; &lt;p&gt;This article looks at the necessary changes and best practices to allow a spec file to build with both GCC and Clang in a variety of cases.&lt;/p&gt; &lt;h2&gt;Preparing a spec file&lt;/h2&gt; &lt;p&gt;In the most basic case, nothing needs to be done. The specification file will automatically pick up whatever compiler the buildroot defines as the default. You can check the default by looking at the value of the &lt;code&gt;%toolchain&lt;/code&gt; macro in the &lt;code&gt;%build&lt;/code&gt; section:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build echo "Toolchain is %toolchain"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Similarly, &lt;code&gt;rpm --eval "%toolchain"&lt;/code&gt; will print the value of the &lt;code&gt;%toolchain&lt;/code&gt; macro. Setting the macro to either "&lt;code&gt;gcc&lt;/code&gt;" or "&lt;code&gt;clang&lt;/code&gt;" will select the toolchain for the given spec file. Setting the macro explicitly is useful if the package needs to be built with a specific toolchain:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%global toolchain clang&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For testing, it might make sense to add a conditional variable to the spec file, so it can be easily built with either toolchain:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;%bcond_with toolchain_clang %if %{with toolchain_clang} %global toolchain clang %else %global toolchain gcc %endif&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Adding the conditional variable should suffice for the minimal setup. In some cases, you might need to make additional changes such as adjusting the &lt;code&gt;BuildRequires&lt;/code&gt;. This is because the &lt;code&gt;%toolchain&lt;/code&gt; macro does not automatically add the appropriate toolchain &lt;code&gt;BuildRequires&lt;/code&gt;. With this setup, building a simple RPM via &lt;code&gt;rpmbuild --with=toolchain_clang&lt;/code&gt; will select the Clang toolchain while not passing anything or passing &lt;code&gt;--with=gcc&lt;/code&gt; will select GCC.&lt;/p&gt; &lt;p&gt;The value of the &lt;code&gt;%toolchain&lt;/code&gt; macro will later determine the value of the various environment variables, as well as what build flags will be passed to the build system. So this macro should be set as early as possible in the file.&lt;/p&gt; &lt;h2&gt;Build systems&lt;/h2&gt; &lt;p&gt;Most C and C++ software projects use a popular build system such as Autotools, CMake, or Meson. These build systems usually try to follow standard practices such as respecting common environment variables, e.g., &lt;code&gt;CC&lt;/code&gt;, &lt;code&gt;CXX&lt;/code&gt;, or &lt;code&gt;CFLAGS&lt;/code&gt;. They are readily supported by RPM via their respective macros, such as &lt;code&gt;%cmake&lt;/code&gt; for CMake, &lt;code&gt;%configure&lt;/code&gt; for Autotools, and &lt;code&gt;%meson&lt;/code&gt; for Meson.&lt;/p&gt; &lt;p&gt;After setting those macros, use &lt;code&gt;%make_build&lt;/code&gt; to actually compile the project. This macro will run &lt;code&gt;make&lt;/code&gt; and pass a few flags for parallel builds. There are special macros to let CMake and Meson build the application, in case the build uses Ninja instead of &lt;code&gt;make&lt;/code&gt;: &lt;code&gt;%cmake_build&lt;/code&gt; and &lt;code&gt;%meson_build&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Just using these macros works for the simplest projects. Some projects, however, do not use any of the common build systems and require special care.&lt;/p&gt; &lt;h2&gt;Hand-written makefiles&lt;/h2&gt; &lt;p&gt;Hand-written makefiles are still relatively common, especially in smaller software projects. Unfortunately, makefiles do not usually respect the environment variables mentioned earlier. This makes it impossible for distributions to inject their hardening C flags or change the compiler in use.&lt;/p&gt; &lt;p&gt;When using hand-written makefiles in a Fedora RPM spec file, the &lt;code&gt;%make_build&lt;/code&gt; macro works as well. However, that does not define the &lt;code&gt;CFLAGS&lt;/code&gt; environment variable and others. &lt;code&gt;%set_build_flags&lt;/code&gt; can be used to set those flags in the makefile:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags %make_build&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;%set_build_flags&lt;/code&gt; also sets the &lt;code&gt;CC&lt;/code&gt; environment variable, so if the compiler needs to be invoked explicitly, &lt;code&gt;$CC&lt;/code&gt; should be used instead of hard-coding &lt;code&gt;gcc&lt;/code&gt; or &lt;code&gt;clang&lt;/code&gt;, or even &lt;code&gt;/usr/bin/cc&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If the provided makefile respects the standard environment variables, you are all set with this approach. If it doesn't, it usually needs to be patched, or the spec file needs to use another environment variable, for example &lt;code&gt;EXTRA_CFLAGS&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags %make_build EXTRA_CFLAGS="$CFLAGS"&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Custom build setups&lt;/h2&gt; &lt;p&gt;This is more of a worst-case scenario for packagers, but many projects still have custom build setups (for example, a shell script that compiles the source code) or no build setup at all.&lt;/p&gt; &lt;p&gt;If the project does not have any build setup, it's easiest to simply use &lt;code&gt;%set_build_flags&lt;/code&gt; again and compile using &lt;code&gt;$CC&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags # Compile main library $CC lib.c -c -o lib.o $CFLAGS $LDFLAGS&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, if the project uses a custom shell script (or something similar), you're out of luck again. Using &lt;code&gt;%set_build_flags&lt;/code&gt; is still a good idea, but you have to inspect the build script to find out what environment variables have to be set or what command-line parameters have to be passed.&lt;/p&gt; &lt;h2&gt;Toolchain-specific changes&lt;/h2&gt; &lt;p&gt;One common problem that spec files run into is that they want to add (or override) flags from the &lt;code&gt;CFLAGS&lt;/code&gt; variable to keep a working build. That is of course easy to do:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags CFLAGS="$CFLAGS -fno-some-flag"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, we assume that a new version of the compiler in use has introduced a new flag that's enabled by default, &lt;code&gt;-fsome-flag&lt;/code&gt;. This flag causes problems when the project in question is compiled, however, so the package maintainer has decided to disable it again by passing &lt;code&gt;-fno-some-flag&lt;/code&gt;. When trying to build this package with a different compiler, it can happen that the build fails because the compiler does not know about &lt;code&gt;-fsome-flag&lt;/code&gt; or &lt;code&gt;-fno-some-flag&lt;/code&gt;. When &lt;code&gt;-fno-some-flag&lt;/code&gt; needs to be passed only for a specific compiler, check the &lt;code&gt;%toolchain&lt;/code&gt; macro for the compiler that requires the flag:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags %if "%toolchain" == "gcc" CFLAGS="$CFLAGS -fno-some-flag" %endif&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As implemented in &lt;code&gt;redhat-rpm-macros&lt;/code&gt;, the &lt;code&gt;%toolchain&lt;/code&gt; macro always evaluates to either "gcc" or "clang." Note that one cannot generally check &lt;code&gt;$CC&lt;/code&gt; the same way, because that variable might point to an obscure binary in cross-compilation scenarios.&lt;/p&gt; &lt;h2&gt;Link-time optimization with Clang&lt;/h2&gt; &lt;p&gt;Link-time optimization (LTO) is enabled by default on Fedora, which means that the &lt;code&gt;$CFLAGS&lt;/code&gt; variable (set by &lt;code&gt;%set_build_flags&lt;/code&gt;) contains a variation of the &lt;code&gt;-flto&lt;/code&gt; flag. The flags used by link-time optimization are saved in the &lt;code&gt;%_lto_cflags&lt;/code&gt; variable. When porting a package to compile with the Clang toolchain, it is important to know that the link-time optimization flags have to be added to the &lt;code&gt;$LDFLAGS&lt;/code&gt; variable as well. &lt;code&gt;%set_build_flags&lt;/code&gt; will take care of that.&lt;/p&gt; &lt;p&gt;When the package needs some non-standard variable, the flags from &lt;code&gt;%_lto_cflags&lt;/code&gt; need to be added manually. In the worst case, link-time optimization has to be disabled entirely. This change is often necessary when the project has some home-grown LTO setup that expects GCC. Disabling link-time optimization can be done by setting &lt;code&gt;%_lto_cflags&lt;/code&gt; to &lt;code&gt;%{nil}&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%global _lto_cflags %{nil}&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Supporting both the GCC and Clang toolchains is easy if the package uses a standard build system and respects standard environment variables. There are still tons of special cases, which can often be handled by checking the &lt;code&gt;%toolchain&lt;/code&gt; macro value and handling the problem depending on the toolchain in use.&lt;/p&gt; &lt;p&gt;Note that currently (as of Fedora 34) all packages in Fedora are built with GCC, but there is a proposal for changing this. See &lt;a href="https://fedoraproject.org/wiki/Changes/CompilerPolicy"&gt;Fedora's Changes/CompilerPolicy page&lt;/a&gt; for details.&lt;/p&gt; &lt;p&gt;Good documentation for these matters is scarce, but I like to look at the macros defined in the &lt;a href="https://github.com/rpm-software-management/rpm/blob/master/macros.in"&gt;RPM repository&lt;/a&gt; as well as the &lt;a href="https://src.fedoraproject.org/rpms/redhat-rpm-config/blob/rawhide/f/macros"&gt;redhat-rpm-config repository&lt;/a&gt;. Looking at the output of &lt;code&gt;rpm --showrc&lt;/code&gt; can also provide insight.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/28/write-toolchain-agnostic-rpm-spec-files-gcc-and-clang" title="Write toolchain-agnostic RPM spec files for GCC and Clang"&gt;Write toolchain-agnostic RPM spec files for GCC and Clang&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6Vh3_fS-NoA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Timm Baeder</dc:creator><dc:date>2021-07-28T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/28/write-toolchain-agnostic-rpm-spec-files-gcc-and-clang</feedburner:origLink></entry><entry><title type="html">Importing and Reusing DMN Models in Red Hat Decision Manager</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2oCeTPPJwg4/reusing-dmn-models.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/07/reusing-dmn-models.html</id><updated>2021-07-28T01:52:40Z</updated><content type="html">Decision Model and Notation (DMN) is a standard established by the Object Management Group (OMG) for describing and modeling operational decisions. DMN decision models can be shared between DMN-compliant platforms and across organizations so that business analysts and business rules developers are unified in designing and implementing DMN decision services. Reusing a DMN model in the scope of a larger decision flow is a common requirement.  did a  earlier this year on this topic. In this presentation, he walks through how we can achieve model reuse through the concept of DMN Decision Service. This article explains the same concept in a step by step manner with a sample use case. A decision service is an invocable function, with well-defined inputs and outputs, that is published as a service for invocation. The decision service can be invoked similarly to a Business Knowledge Model (BKM) node from within the model itself if necessary. The standard usage of the decision service node is for it to be leveraged for re-use from an external application or a business process. The decision service allows for reusability by providing an interface for invocation from another DMN model. To know more about how to implement reusable decisions, check out for a step by step explanation with an example. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2oCeTPPJwg4" height="1" width="1" alt=""/&gt;</content><dc:creator>Sadhana Nandakumar</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/reusing-dmn-models.html</feedburner:origLink></entry></feed>
